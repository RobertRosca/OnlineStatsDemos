{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Approximation Simulations (Linear Regression)\n",
    "\n",
    "It is difficult to compare stochastic approximation algorithms.  The following interactive example shows the behavior of several algorithms under a variety of different conditions for a linear regression model.  You'll notice there is no universal \"winner\", although some methods are more robust to the learning rate than others.\n",
    "\n",
    "## Singleton Objective Function\n",
    "\n",
    "$$f_t(\\beta) = \\frac{1}{2}(y_t - x_t^T\\beta)^2$$\n",
    "\n",
    "$$g_t = \\nabla f_t(\\beta) = -(y_t - x_t^T\\beta)x_t$$\n",
    "\n",
    "## Conditions\n",
    "\n",
    "- Each algorithm uses a learning rate paramerized as \n",
    "\n",
    "$$\\gamma_t = \\frac{1}{t^r},\\quad r\\in\\{.5, .7, .9\\}$$.\n",
    "\n",
    "- The data is generated from a linear model:\n",
    "\n",
    "    $$y_t = x_t^T\\beta + \\epsilon_t,$$\n",
    "\n",
    "    where:\n",
    "    - $\\epsilon_t\\sim N(0, \\sigma), \\quad \\sigma \\in \\{1, 5, 20, 50\\}$\n",
    "    - $\\beta$ is one of the following:\n",
    "        - $(-1,\\ldots,1)$\n",
    "        - $(1,\\ldots,1)$\n",
    "        - $(10,\\ldots,10)$\n",
    "        \n",
    "## The Visualization\n",
    "\n",
    "What is being plotted below is the value of the **population objective**\n",
    "\n",
    "$$f(\\beta) = \\frac{1}{n}\\sum_t f_t(\\beta),$$\n",
    "\n",
    "relative to the OLS solution, evaluated at the current estimate $\\hat{\\beta}^{(t)}_{SGD}$, $\\hat{\\beta}^{(t)}_{ADAGRAD},$ etc.  For example, the SGD line above a point $t$ on the x-axis represents\n",
    "\n",
    "$$f(\\beta^{(t)}_{SGD}) - f(\\hat\\beta_{OLS}).$$\n",
    "\n",
    "Since the values are relative to the *best* solution, as the lines approach zero they approach the optimum of the population objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OnlineStats, Interact, Plots, Random\n",
    "gr(fmt=:png)\n",
    "\n",
    "Losses = [L1DistLoss(), .5L2DistLoss(), LogitMarginLoss()]\n",
    "Alg1 = [SGD(), ADAGRAD(), RMSPROP(), ADAM(), ADAMAX(), OMAS(), OMAP(), MSPI()]\n",
    "Alg2 = [SGD(), ADAGRAD(), RMSPROP(), ADAM(), ADAMAX(), OMAS(), OMAP(), MSPI()]\n",
    "\n",
    "σs = [1, 2, 5, 10, 50]\n",
    "βs = [collect(range(-1,stop=1,length=10)), ones(10), fill(10.0, 10)]\n",
    "Loss = .5 * L2DistLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@manipulate for A1 in Alg1, A2 in Alg2, σ in σs, β in βs, seed in 1:100, r in .5:.02:1\n",
    "    Random.seed!(seed)\n",
    "    n, p = 1000, 10\n",
    "    x = randn(n, p)\n",
    "    y = x * β + σ * randn(n)\n",
    "\n",
    "    out = zeros(n, 2)\n",
    "    o1 = StatLearn(p, Loss, A1, rate=LearningRate(r))\n",
    "    o2 = StatLearn(p, Loss, A2, rate=LearningRate(r))\n",
    "    s = Series(o1, o2)\n",
    "    \n",
    "    for i in 1:n\n",
    "        xi = @view x[i, :]\n",
    "        yi = y[i]\n",
    "        fit!(s, (xi, yi))\n",
    "        out[i, 1] = OnlineStats.objective(o1, x, y)\n",
    "        out[i, 2] = OnlineStats.objective(o2, x, y)\n",
    "    end\n",
    "    ymax = 2minimum(out[end,1])\n",
    "    plot(out, ylim = (0, ymax), label = [string(A1) string(A2)], w = 2, title = \"Learn Rate = $r\",\n",
    "        linestyle = :auto)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
